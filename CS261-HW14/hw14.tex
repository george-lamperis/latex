\RequirePackage[l2tabu, orthodox]{nag}

\documentclass[letterpaper, 12pt, oneside]{memoir}
\usepackage{amsmath,amsthm,amssymb,mathtools}
\usepackage{float} % H option to place tables exactly

% Memoir configuration
\pagestyle{plain}
%\setlength{\parindent}{0cm}
\setlength{\parskip}{1ex}

% Omit chapter numbering
\counterwithout{section}{chapter}

% ------------------------------------------------------------------------------
% Begin Document
% ------------------------------------------------------------------------------
\title{CS 261 - Homework w14}
\author{George Lamperis}
\date{}

\begin{document}
\maketitle

\section{Data}

\subsection{blocking.cc}
For every test with blocking.cc, we have 4,200,000 accesses. This data 
was taken using a direct mapped cache, a blocking factor of 20, and a matrix
size of 100.

% ------------------------------------------------------------------------------
% blocking.cc
% ------------------------------------------------------------------------------
\begin{table}[H]
\centering
\begin{tabular}{c|c|c|r}
    Cache Size (KB)  & Block Size & Miss Rate & Total time \\ \hline 
    4  & 1 & 4.5    & 62865020 \\
    4  & 2 & 3.0    & 56889120 \\
    4  & 4 & 2.3    & 55645940 \\
    4  & 8 & 2.8    & 63088080 \\ \hline
    8  & 1 & 3.4    & 57504170 \\
    8  & 2 & 2.0    & 51951240 \\
    8  & 4 & 1.4    & 49987280 \\
    8  & 8 & 1.4    & 52446660 \\ \hline
    16 & 1 & 2.9    & 55605900 \\
    16 & 2 & 1.6    & 50142720 \\
    16 & 4 & 1.0    & 47808600 \\
    16 & 8 & 0.9    & 48560640 \\ \hline
    32 & 1 & 2.0    & 51315570 \\
    32 & 2 & 1.1    & 47424360 \\
    32 & 4 & 0.6    & 45652320 \\
    32 & 8 & 0.5    & 45765420 \\
\end{tabular}
\caption{blocking.cc}
\end{table}

\subsection{obvious.cc}
For every test with obvious.cc, we have 4,040,000 accesses.

Matrix size: 100
% ------------------------------------------------------------------------------
% obvious.cc
% ------------------------------------------------------------------------------
\begin{table}[H]
\centering
\begin{tabular}{c|c|c|r}
    Cache Size (KB)  & Block Size & Miss Rate & Total time \\ \hline 
    4  & 1 & 27.7   & 163548630 \\
    4  & 2 & 16.5   & 120387320 \\
    4  & 4 & 10.9   & 102084280 \\
    4  & 8 & 20.6   & 190459340 \\ \hline
    8  & 1 & 26.5   & 158077340 \\
    8  & 2 & 14.5   & 110739680 \\
    8  & 4 & 8.5    & 88656040 \\
    8  & 8 & 18.8   & 177242300 \\ \hline
    16 & 1 & 25.8   & 155259360 \\
    16 & 2 & 13.5   & 106037600 \\
    16 & 4 & 7.4    & 82203580 \\
    16 & 8 & 18.0   & 171396080 \\ \hline
    32 & 1 & 10.5   & 87068490 \\
    32 & 2 & 5.6    & 67332680 \\
    32 & 4 & 3.1    & 57918480 \\
    32 & 8 & 1.9    & 54344780 \\
\end{tabular}
\caption{obvious.cc}
\end{table}

\subsection{Hardware}

Matrix size: 1000
Blocking factor: 50
\begin{table}[H]
\centering
\begin{tabular}{c|c}
    Program & Execution Time (seconds) \\ \hline
    blocking.cc        & 8.26 \\
    obvious.cc         & 15.04 \\
    blocking.cc (-O2)  & 1.20 \\
    obvious.cc  (-O2)  & 16.39
\end{tabular}
\caption{hardware data}
\end{table}

\subsection{Associativity}
We'll use a blcok size of 1

\begin{table}[H]
\centering
\begin{tabular}{c|c|r}
    Associativity & Miss Rate (blocking.cc) & Total Time \\ \hline
    1           & 4.5   & 62865020 \\
    2           & 4.2   & 70003210 \\
    4           & 2.6   & 79303410 \\
    8           & 2.6   & 112900110 \\
    16          & 2.6   & 180100110 \\
    1024 (fully) & 2.6   & 8647300110 \\ \hline
    1           & 2.0   & 51315570 \\
    2           & 2.0   & 59717880 \\
    4           & 1.8   & 75466170 \\
    8           & 1.7   & 108500110 \\
    16          & 1.7   & 175700110 \\
    8192 (fully) & 1.7  & 68854100110 \\
\end{tabular}
\caption{blocking.cc with 4 kb cache (top) and 32 kb cache (bottom)}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c|c|r}
    Associativity & Miss Rate (blocking.cc) & Total Time \\ \hline
    1           & 27.7  & 163548630 \\
    2           & 25.3  & 160724110 \\
    4           & 25.2  & 176840660 \\
    8           & 25.2  & 209160110 \\
    16          & 25.2  & 273800110 \\
    1024 (fully) & 25.2  & 8418440110 \\ \hline
    1           & 10.5  & 87068490 \\
    2           & 14.9  & 114713090 \\
    4           & 23.2  & 167921640 \\
    8           & 25.2  & 209160110 \\
    16          & 25.2  & 273800110 \\
    8192 (fully) & 25.2 & 66335880110 \\
\end{tabular}
\caption{obvious.cc with 4 kb cache (top) and 32 kb cache (bottom)}
\end{table}

\subsection{Blocking factor}
block size 1

\begin{table}[H]
\centering
\begin{tabular}{c|c|c}
    Blocking factor & Miss Rate (4 kb) & Miss Rate (32 kb) \\ \hline
    1   & 14.2  & 5.3 \\
    10  & 5.5   & 2.3 \\
    20  & 4.5   & 2.0 \\
    30  & 10.4  & 2.0 \\
    40  & 21.0  & 2.0 \\
    50  & 26.7  & 1.8 \\
    60  & 26.9  & 1.9 \\
    70  & 27.1  & 1.9 \\
    80  & 27.3  & 1.9 \\
    90  & 27.9  & 5.7 \\
\end{tabular}
\caption{different blocking factors with matrix size 100, direct mapped cache, and a 1 word block size}
\end{table}

% ------------------------------------------------------------------------------
% Questions
% ------------------------------------------------------------------------------
\section{Discussion Questions}


\subsection{1}
\textbf{A comparison of blocking.cc and obvious.cc in terms of their algorithmic 
complexity and cache-friendliness. Why did increasing the block size sometimes 
increase the miss rate?}

The obvious algorithm appears less complex, with only three nested loops, as
opposed to five nested using the blocking algorithm. One might think that
fewer nested loops would imply a faster execution time. However, this was
not the case.

The matrix $b$ is especially cache freindly, allowing us to exploit spacial
locality. On the first iteration of the inner-most loop, we must fetch
$b[i][k]$. As a result $b[i][k+1]$, $b[i][k+2]$, and so on will be stored in 
the cache as well. Therefore, we have quick access to this memory on successive
iterations of the loop. The line $ a[i][j] = a[i][j] + r$  exibits spacial
locality as well. As with the matrix $b$ above, other values in the same row 
as $a[i][j]$ will also be cached. 

Increasing the block size decreases the miss rate by increasing the ability to
exploit spacial locality. When you request one byte of memory, the
entire block in which that byte is stored gets loaded into the cache. Because 
of this, when we request one entry of the matrix, other nearby matrix entries
will be cached for later use. On successive iterations, the data we need to use
is already in the cache.

However, a larger block size means that a cache miss is more expensive. When
there is a cache miss, there is a larger about of data to be evicted and a
larger amount of new data to store in the cache. It takes more time to transfer
a larger amount of data. From our experimental data, it seems that a block size
of 4 seems to be optimal for smaller sized caches.


\subsection{2}
\textbf{What would be the optimal blockingFactor to use for matrix
multiplication?}

As blocking factor increases, we can fit a larger chunk of our matrices in the
cache, and exploit spacial locality. The innermost loop can run quickly because
all the data it needs is stored in the cache. However, if the blocking factor
is too large, we lose the ability to exploit spacial locality because our entire
block doesn't fit in the cache. For example, if the block size is too
large, we end up caching too much of matrix $b$. Then, when we want data from
matrix $c$, we must evict some of the data we cached from matrix $b$ to make
room for data from matrix $c$.

Looking at our experimental data, the optimal blocking factor for a matrix
of size 100 is 20.


\subsection{3}
\textbf{A discussion of the significance of the results, given that all of the
simulated caches were considerably smaller than current hardware caches.}

Although our cache size is small, so is the program we tested it with. Given a
larger cache and a more complicated program, we could produce simlar results.

Additonally, real hardware has multiple levels of cache. The highest level of
cache L1 is in fact quite small. The i7 processor has 64 kb of cache memory.
Therefore, our test values are actually very close to the size of L1 cache in 
a modern processor.

\subsection{4}
\textbf{Does increased associativity always reduce the miss rate?}

Increasing associativity tends to improve the miss rate by reducing the rate of
conflict misses. From our experimental data, we see that increasing
associativity improves the miss rate up until 4-way associativity. We also see
that with 8-way associativity and higher, the miss rate stays constant, but the
execution time skyrockets. We can conclude that increasing associativity reaches
the point of diminishing returns quickly.

This is because a higher associativity is difficult to implement. Because of
the complexity of higher associative cache, it is expensive to implement the
hardware, and difficult to make fast. A fully associative is only feasible with
really small amount of memory.

\subsection{5}
\textbf{How are caches implemented in hardware? Why does the simulator insist
that the block size, and the number of sets, must be a whole power of two? Is it always
necessary to store the entire block address in the cache for identification
purposes?}


\subsection{6}
\textbf{Why does a main memory access involve a significant, fixed overhead? How
can main memory systems be designed to support caches?}


\subsection{7}
\textbf{What are the attractions of a multilevel caching strategy?}

By keeping instruction data and program data in separate caches, we can
guarantee that there are no conflict misses between instruction data and
program data.

Although L2 cache is slower than L1 cache, it is orders of
magnitude faster than main memory. Still an improvement.



\end{document}
